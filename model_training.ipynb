{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25060d0-7973-4a8c-8a14-73da1c77a31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Install Required Libraries\n",
    "# Ensure you have the following libraries installed:\n",
    "# bash\n",
    "# pip install transformers datasets torch scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaeaca80-e254-4929-9a12-9d8f4953eca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentences: 1\n",
      "Dataset is too small to split. Using all data for training.\n",
      "\n",
      "Dataset Statistics:\n",
      "Number of unique tokens: 3007\n",
      "Number of unique tags: 15\n",
      "Average sentence length: 15777.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This is script prepares a dataset for Named Entity Recognition (NER) tasks by loading a CSV file, \n",
    "tokenizing the text into sentences, and splitting the data into training and validation sets.\n",
    "'''\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"my_dataset.csv\"  # Path to your dataset\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Define the custom tags and map them to integers\n",
    "custom_tags = [\n",
    "    \"O\", \"I-ORG\", \"I-PERSON\", \"B-DISEASE\", \"B-ORG\", \"B-PERSON\",\n",
    "    \"I-DISEASE\", \"B-CHEMICAL\", \"B-GPE\", \"I-CHEMICAL\",\n",
    "    \"I-DATE\", \"B-DATE\", \"I-GPE\", \"B-LOC\", \"I-LOC\"\n",
    "]\n",
    "tag_to_id = {tag: idx for idx, tag in enumerate(custom_tags)}\n",
    "id_to_tag = {idx: tag for tag, idx in tag_to_id.items()}\n",
    "\n",
    "# Map NER tags to their integer IDs\n",
    "data['NER'] = data['NER'].map(tag_to_id)\n",
    "\n",
    "# Prepare tokens and tags for sentence grouping\n",
    "all_tokens = data['Token'].tolist()\n",
    "all_tags = data['NER'].tolist()\n",
    "\n",
    "# Tokenize text into sentences\n",
    "text = \" \".join(all_tokens)  # Combine tokens into a single string\n",
    "sent_tokens = sent_tokenize(text)  # Split text into sentences\n",
    "\n",
    "# Group tokens and tags by sentences\n",
    "sentences = []\n",
    "ner_tags = []\n",
    "token_index = 0\n",
    "\n",
    "for sent in sent_tokens:\n",
    "    words = sent.split()  # Split sentence into words\n",
    "    current_sentence = []\n",
    "    current_tags = []\n",
    "\n",
    "    for word in words:\n",
    "        if token_index < len(all_tokens):  # Ensure index is within bounds\n",
    "            current_sentence.append(word)\n",
    "            current_tags.append(all_tags[token_index])\n",
    "            token_index += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    if current_sentence:  # Add non-empty sentences\n",
    "        sentences.append(current_sentence)\n",
    "        ner_tags.append(current_tags)\n",
    "\n",
    "# Print initial dataset size\n",
    "print(f\"Total number of sentences: {len(sentences)}\")\n",
    "\n",
    "# Check if we have enough data to split\n",
    "if len(sentences) > 1:\n",
    "    # Split into training and validation sets\n",
    "    train_tokens, val_tokens, train_tags, val_tags = train_test_split(\n",
    "        sentences, ner_tags, train_size=0.8, random_state=42)\n",
    "    \n",
    "    # Display dataset sizes\n",
    "    print(f\"Number of training sentences: {len(train_tokens)}\")\n",
    "    print(f\"Number of validation sentences: {len(val_tokens)}\")\n",
    "\n",
    "    # Prepare dataset dictionaries\n",
    "    train_data = {\"tokens\": train_tokens, \"ner_tags\": train_tags}\n",
    "    val_data = {\"tokens\": val_tokens, \"ner_tags\": val_tags}\n",
    "\n",
    "else:\n",
    "    print(\"Dataset is too small to split. Using all data for training.\")\n",
    "    # Use all data for training\n",
    "    train_data = {\"tokens\": sentences, \"ner_tags\": ner_tags}\n",
    "    val_data = {\"tokens\": [], \"ner_tags\": []}\n",
    "\n",
    "# Print some statistics about the data\n",
    "print(\"\\nDataset Statistics:\")\n",
    "print(f\"Number of unique tokens: {len(set([token for sent in sentences for token in sent]))}\")\n",
    "print(f\"Number of unique tags: {len(set([tag for tags in ner_tags for tag in tags]))}\")\n",
    "print(f\"Average sentence length: {sum(len(sent) for sent in sentences)/len(sentences) if sentences else 0:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7da5628-c0ff-4a2d-aae2-b1fe806ef6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Convert to Hugging Face Dataset\n",
    "# Hugging Face expects datasets in its datasets format.\n",
    "'''\n",
    "This script prepares a dataset for Named Entity Recognition (NER) tasks by loading a CSV file, \n",
    "tokenizing the text into sentences, splitting the data into training and validation sets, \n",
    "and converting the data into the Hugging Face Dataset format.\n",
    "'''\n",
    "from datasets import Dataset\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "train_dataset = Dataset.from_dict(train_data)\n",
    "val_dataset = Dataset.from_dict(val_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c7d1833-657d-4923-8893-e987b955e9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 4. Load a Pre-Trained Transformer\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = \"bert-base-cased\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name, num_labels=len(tag_to_id)\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72ca77fd-e3bb-44fb-84a4-cb116bc33b3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=15, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# my model \n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caabc2b7-f92a-427e-9406-8182b53b8e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Tokenize and Align Labels\n",
    "# tokenize input text and align the corresponding labels. \n",
    "#This is a crucial step in preparing data for training a named entity recognition (NER) model.\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples['tokens'], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['ner_tags']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = [-100 if word_id is None else label[word_id] for word_id in word_ids]\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_train = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_val = val_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ce0366-f3a6-474a-b8c1-b77ef22d9ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Define Training Arguments\n",
    "# Set hyperparameters and configurations.\n",
    "\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ner_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fc15ce-05ee-4e98-84a4-ceae730264ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules from transformers and sklearn\n",
    "from transformers import DataCollatorForTokenClassification, Trainer\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Data collator for token classification; handles padding sequences dynamically\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "# Function to compute evaluation metrics\n",
    "def compute_metrics(pred):\n",
    "    # Extract predictions and labels from the output\n",
    "    predictions, labels = pred\n",
    "    # Apply argmax to predictions along the last dimension to get predicted classes\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove the ignored index (-100) from labels for true evaluations\n",
    "    true_labels = [[l for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [p for (p, l) in zip(prediction, label) if l != -100]  # Align predictions with labels\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    # Flatten the labels and predictions for metrics calculation\n",
    "    flat_labels = [item for sublist in true_labels for item in sublist]\n",
    "    flat_predictions = [item for sublist in true_predictions for item in sublist]\n",
    "    \n",
    "    # Generate a classification report with precision, recall, and F1-score\n",
    "    report = classification_report(flat_labels, flat_predictions, output_dict=True)\n",
    "    return {\n",
    "        \"precision\": report[\"weighted avg\"][\"precision\"],  # Weighted average precision\n",
    "        \"recall\": report[\"weighted avg\"][\"recall\"],        # Weighted average recall\n",
    "        \"f1\": report[\"weighted avg\"][\"f1-score\"],          # Weighted average F1-score\n",
    "    }\n",
    "\n",
    "# Define the Trainer instance for model training and evaluation\n",
    "trainer = Trainer(\n",
    "    model=model,                         # The model to be trained\n",
    "    args=training_args,                  # Training arguments (e.g., epochs, batch size)\n",
    "    train_dataset=tokenized_train,       # Tokenized training dataset\n",
    "    eval_dataset=tokenized_val,          # Tokenized validation dataset\n",
    "    tokenizer=tokenizer,                 # Tokenizer for pre-processing\n",
    "    data_collator=data_collator,         # Data collator for padding sequences\n",
    "    compute_metrics=compute_metrics,     # Custom metric computation function\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1d7d28-dd29-4b52-98e9-ffc2a207a6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Train the Model\n",
    "# Start fine-tuning the model.\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55a3c3b-e69e-4224-ad4c-4e6c962f4762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Evaluate the Model\n",
    "# Evaluate on the validation dataset.\n",
    "\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a12758-0737-455d-82ca-40f5fe3aec6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Save the Fine-Tuned Model\n",
    "# Save the model for future use.\n",
    "\n",
    "\n",
    "model.save_pretrained(\"./fine_tuned_ner_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_ner_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9cdddb-a500-4195-9a2f-d653f5fde333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Inference with Fine-Tuned Model\n",
    "# Use the fine-tuned model for predictions.\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the fine-tuned model\n",
    "ner_pipeline = pipeline(\n",
    "    \"ner\", model=\"./fine_tuned_ner_model\", tokenizer=\"./fine_tuned_ner_model\", aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "# Predict entities\n",
    "text = \"Hassen diagnosed John with COVID-19 at MG\"\n",
    "entities = ner_pipeline(text)\n",
    "print(entities)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
